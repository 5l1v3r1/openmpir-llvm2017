\documentclass[sigconf]{acmart}
\newcommand{\wmnote}[1]{{\scriptsize \color{red} [[ Billy: #1]]}}
\newcommand{\gsnote}[1]{{\scriptsize \color{blue} [[ George: #1]]}}

\usepackage{booktabs} % For formal tables
\usepackage{multicol}
\usepackage{minted}
\graphicspath{{./figs/}}

% Dejavu fonts have unicode character support
% \usepackage{DejaVuSansMono}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{listings}
\usepackage{subfigure}
\newcommand{\figlabel}[1]   {\label{fig:#1}}
\newcommand{\figref}[1]         {Figure~\ref{fig:#1}}
\newcommand{\figreftwo}[2]      {Figures \ref{fig:#1} and~\ref{fig:#2}}
\newcommand{\figrefthree}[3]      {Figures \ref{fig:#1}, \ref{fig:#2} and~\ref{fig:#3}}
\newcommand{\subfiglabel}[1]    {\textbf{#1}}
\newcommand{\subfigcap}[1]      {\textbf{~~#1}}
\newcommand{\subfigref}[2]      {Figure~\ref{fig:#1}#2}
\newcommand{\subfigreftwo}[3]      {Figures~\ref{fig:#1}#2 and~\ref{fig:#1}#3}

\definecolor{CodeColor}{RGB}{98,39,26}
\def\code{\lstinline[basicstyle=\ttfamily\color{CodeColor}]} 


\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage{autofe}

\lstset{numbers=left,
        numberstyle=\tiny,
        numbersep=5pt,
        xleftmargin=0.1in
}

% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\settopmatter{printacmref=false}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
\acmDOI{10.475/123_4}

% ISBN
\acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[LLVM in HPC'17]{LLVM in HPC: Fourth Workshop on the LLVM Compiler Infrastructure in HPC}{November 2017}{Denver Colorado, USA} 
\acmYear{2017}
\copyrightyear{2017}

%\acmPrice{15.00}

\begin{document}
\lstset{basicstyle=\tt\small, language=C, 
morekeywords={omp, for, pragma, omp, task, taskwait}}

\title{OpenMPIR}
\subtitle{Implementing OpenMP tasks with Tapir}
\author{George Stelle}
\affiliation{\institution{Los Alamos National Laboratory}}
\email{stelleg@lanl.gov}

\author{William S. Moses}
\affiliation{\institution{MIT CSAIL}}
\email{wmoses@mit.edu}

\author{Stephen Olivier}
\affiliation{\institution{Center for Computing Research \\ Sandia National Laboratories}}
\email{slolivi@sandia.gov}

\author{Pat McCormick}
\affiliation{\institution{Los Alamos National Laboratory}}
\email{pat@lanl.gov}

\begin{abstract}
Optimizing compilers for task-level parallelism are still in their infancy.
This work implements a frontend which compiles a subset of OpenMP tasks to
Tapir, an extension to LLVM that represents fork-join parallelism. This enables
analyses and optimizations previously inaccessible to OpenMP codes, as well as
the ability to target other runtimes. Using a Cilk runtime backend, we compare
to existing OpenMP implementations. Initial performance results for the
Barcelona OpenMP tasking suite and show performance improvements over existing
OpenMP compiler implementations. 
\end{abstract}

\maketitle

\section{Introduction} \label{Sec:Introduction}

When writing task-parallel programs today, one has a large selection of
programming models. Unfortunately, despite having significant overlap in
semantics, parallel programming models like OpenMP, Cilk, Kokkos, HPX, Charm++,
Qthreads, pthreads, MPI, Chapel, UPC, etc. have virtually no ability to
interoperate, both at compile time and run time. In addition, many of these
programming models are implemented as libraries, and therefore have no compiler
support. There has been some recent work on specializing compilers to reason
about parallel programming libraries \cite{Moss_2016}, but fragmentation among
models presents a significant challenge for compiler writers, which must choose
a single model to analyze and optimize. An ideal solution would be some form of
common representation of parallelism.

Compilers use internal representations \textbf{(IRs)} for implementing the
majority of their analyses and optimizations \cite{llvm}. Historically, a major
drawback for parallel programs is that IRs haven't had the ability to
represent, and therefore reason about, parallelism.  This has prevented the
application of common optimizations to concurrent code.  The recent work of
Schardl et al. \cite{tapir} has shown that real world compiler IRs can be
extended to represent fork-join parallelism. By extending the LLVM instruction
set with three instructions, they were able to capture all of the semantics to
implement the semantics of Cilk, an extension to C. They suggested that a
similar approach could be taken with OpenMP tasks.

In this work, we put that suggestion to the test, implementing OpenMP tasks by
compiling them to Tapir instructions. This enables us to target non-openmp
runtimes and compare performance to existing OpenMP implementations. For this
paper, our prototype implementation uses a Cilk runtime as a backend, and we run
our prototype implementation on the Barcelona OpenMP task suite
\cite{barcelona}. We discuss the kinds of optimizations this enables, and how
the work could be extended to include other parallelism constructs and semantics
in OpenMP. We also discuss the possibility of extending this work to other
programming models, which would help to alleviate some of the fragmentation
issues discussed above. 

The contributions of this work include: 

\begin{itemize}
\item A prototype frontend from OpenMP task constructs to Tapir IR
\item Benchmarks of the implementation using a Cilk runtime backend, showing
performance improvements over existing OpenMP implementations
\item An initial investigation into the reasons for the performance improvement
\end{itemize}

Our hope for this paper is that it moves the community towards a shared IR that
can be used for many programming models. This work presented in this paper only
represents a small step in that direction, and there is still a herculean amount
of work required. We argue that the benefits vastly outweigh the costs, and
hope to see more work follow suit. 

\subsection{Outline}

The remainder of the paper is organized as follows: In Section~\ref{Sec:Background} 
we give background and motivation for the paper. Following that, we describe OpenMP
tasking in Section~\ref{Sec:OpenMP}, and Tapir in Section~\ref{Sec:Tapir}. We then 
discuss the implementation, and how we compile OpenMP task constructs to Tapir IR in
Section~\ref{Sec:Implementation}. In Section~\ref{Sec:Evaluation} we describe the 
evaluation setup, including what implementations we compare to and how. We discuss
the results of evaluation in Section~\ref{Sec:Results}, including possible
explanations for discrepancies. We discuss the implications of the results ans the
significance of this work in a large context in~\ref{Sec:Discussion}. Finally,
we cover some of the many ways the work can be extended in
Section~\ref{Sec:Future}, and conclude in Section~\ref{Sec:Conclusion}.

\section{Background} \label{Sec:Background}

Internal representations (IRs) are a crucial part of modern compilers.  By
representing code in a generic, hardware-agnostic format, they allow both
multiple frontends for different source languages, and multiple backends for
different hardware, to all take advantage of a series of compiler optimizations
and analyses. 

\wmnote{This feels like we're trying to convince someone about LLVM: perhaps
LLVM has gained traction in the academic community for both its simplicity and
relatively painless modification} 
\gsnote{I wanted to give a brief nod to the importance of LLVM, given the venue
(LLVM in HPC). Is this any more satisfactory?} 
Among IRs, LLVM is arguably the most ubiquitous \cite{lattner2004llvm}. Winner
of the 2012 ACM Software System Award, it is in use by frontends ranging from
Haskell to C++, and backends ranging from x86 to PTX \cite{lattner2004llvm}.
Implementing a simple, extensible single static assignment IR, LLVM enables
powerful optimizations that are effective for a wide range of programming
languages and hardware \cite{lattner2004llvm}. 

Historically, LLVM has had a sequential semantics: there is no explicit notion
of concurrent or parallel processes. This has resulted in existing compilers
treating parallelism constructs as thin wrappers for calls into runtime
systems. These calls are infeasible for the compiler to reason about, due to
the complexity and flexibility of runtimes being called. This situation was
recently remedied by Schardl et al.  \cite{tapir}. By adding three instructions
to LLVM, Tapir enables analyses and optimizations of parallel code. We describe
Tapir in detail in Section~\ref{Sec:Tapir}. For a more complete description see
\cite{tapir}. 

Arguably the most ubiquitous parallel programming model for on-node parallelism
in HPC, OpenMP has grown from being a way to implement parallel for loops to an
enormous and complex specification for parallelism. This paper focuses on a 
relatively recent addition to the OpenMP specification: the \texttt{task}
and \texttt{taskwait} constructs. A detailed description of these constructs
is presented in Section~\ref{Sec:OpenMP}. For a more complete description see
\cite{spec}.

\subsection{Fragmentation}

In this section we further discuss a significant challenge in HPC:
fragmentation of parallel programming models. There are many ways to write
parallel programs \cite{spec, qthreads, chapel, cilk, kokkos, legion}. From
libraries, to language extensions, to standalone languages, to combinations of
the above, fragmentation of parallel programming models is a problem that is
only getting worse. This means that when writing tooling, optimizations, or
analyses, one has to choose which model to target. One common difficulty among
all of these is that reasoning about parallel programs is hard, and compilers
can help. This is exactly analogous the problem LLVM solves: compiler
optimizations are hard , so sharing them is valuable. Tapir is a demonstration
that the the LLVM approach to compilation of serial code can be effectively
extended to the case of parallel code. 

With fragmentation comes questions of compatibility, or composability, of 
different programming models. To illustrate this issue, consider the example of
a program using a library that uses OpenMP to implement parallelism internally. 
If said program uses a different programming model for parallelism, e.g. Cilk
, there is currently no good solution for ensuring that the two different backends
will cooperate with management of hardware resources. 

Addressing these concerns this is a major motivation for our use of Tapir.
By standardising an IR, in addition to easing the development of optimizations,
one avoids duplicating work across different implementations. It also enables
the possibility of multiple programming models coexisting peacefully. We'll
return to this issue later in Section~\ref{Sec:Discussion} when discussing
future work.

\section{Tapir} \label{Sec:Tapir}
Tapir is an extension to LLVM proposed by Schardl, Moses, and Leiserson
aiming to resolve issues regarding optimizing parallel code. Prior to
the introduction of Tapir, compilers would represent parallel programs
by directly translating their syntax to opaque runtime calls. This allowed
for compilers to support parallel programs, but meant that traditional 
optimizations such as code motion \cite{} or common-subexpression elimination \cite{}
weren't able to reason about parallel programs. This often led to parallel
programs running significantly slower than expected. In the Tapir project,
the authors show that it is possible to represent and optimize fork-join
parallel programs with relatively minimal modifications to the compiler: namely
the introduction of three instructions designed to interface well with an
existing compiler.

\subsection{Compilation without Tapir}

\setminted{fontsize=\small,baselinestretch=1}
\begin{figure}[t]
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}l@{}l}

\subfiglabel{a}\\
\begin{lstlisting}
void search(int low, int high) {
  if (low == high) search_base(low);
  else {
    #pragma omp task
    search(low, (low+high)/2);
    #pragma omp task
    search((low+high)/2 + 1, high);
    #pragma omp taskwait
  } 
}
\end{lstlisting}
\\
\subfiglabel{b}\\
\begin{lstlisting}
void search(int low, int high) {
  if (low == high) search_base(low);
  else {
    int mid = (low+high)/2;
    #pragma omp task
    search(low, mid);
    #pragma omp task
    search(mid + 1, high);
    #pragma omp taskwait
  } 
}
\end{lstlisting}
\vspace{0.1ex}
\end{tabular*}

\caption[Example of common-subexpression elimination on an OpenMP
    program.]{Example of common-subexpression elimination on an OpenMP
    program.  \subfigcap{a}~The function \code{search}, which uses
    parallel divide-and-conquer to apply the function
    \code{search_base} to every integer in the closed interval
    [\code{low}, \code{high}].  \subfigcap{b}~An optimized version of
    \code{search}, where the common subexpression \code{(low+high)/2}
    of the original version
    is computed only once and stored in the variable \code{mid} in
    the optimized version.}
  \label{fig:search}
\end{figure}

Consider the first code segment defined in \figref{search}.
Here, we find a simple OpenMP program which performs a divide-and-conquer search.
However, as written, there is a simple optimization that can be performed --
namely, that the value \code{(low+high)/2} may be computed once before either
parallel task, allowing the program to avoid some redundant computation. On
the serial version of this program, optimizations like this are performed
automatically. However, without the use of Tapir, these sorts of optimizations
are not run on parallel programs.

If one were to compile this program using the traditional technique of converting
to runtime calls, one would find a program similar to \figref{runtime_calls}.
The OpenMP pragmas are effectively treated as syntactic sugar for such runtime calls
because vanilla LLVM has no concise way to represent parallel programs.
These runtime calls obfuscate the program, making it impossible to analyze or
optimize parallel code.

\begin{figure}[t]
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}l@{}l}
\begin{lstlisting}
void task_1(int* clos) {
  search(*clos[0], (*clos[0] + *clos[1])/2);
}

void task_2(int* clos) {
  search(*clos[0], (*clos[0] + *clos[1])/2);
}

void search(int low, int high) {
  if (low == high) search_base(low);
  else {
    int* closure_1[] = {&low, &high};
    omp_run_task(task_1, closure_1);
    int* closure_2[] = {&low, &high};
    omp_run_task(task_2, closure_2);
    omp_taskwait();
  } 
}
\end{lstlisting}
\vspace{0.1ex}
\end{tabular*}

\caption[Simplified compilation of the unoptimized search code from \figref{search}.]{Simplified compilation of the unoptimized search code from \figref{search}.  \subfigcap{a}~The function \code{search}. The parallel tasks are moved into their own functions and accompanying closures. These functions are then passed to OpenMP runtime calls. This system obfuscates the program, making it impossible for an optimizer to reason about what is happening.}
  \label{fig:runtime_calls}
\end{figure}

\subsection{Tapir Internals}
Tapir is a compelling solution to this problem because it allows existing
analysis and optimizations to work on parallel programs without much modification.
Tapir does this by extending LLVM's instruction set to natively represent
fork-join parallel programs in a way that 

... TODO ...


Specifically, Tapir introduces the \texttt{detach}, \texttt{reattach}, and
\texttt{sync} instructions. \wmnote{How in detail should I go here? I can talk
about these instructions on a broad level as a target for parallel tasks, or
more concretely define what they do. However, we don't seem discuss CFG's
before this so I'm not sure what level of introduction you're thinking of.}
\gsnote{I was thinking roughly a page should be enough. Maybe a summary of what
Tapir is followed by an example of an optimization it enables. I like both the
CSE and tail-call optimization examples from your PPoPP paper. Feel free to 
define CFG and any other terms you need to. If that's too much to ask on such 
short notice just let me know and I'll help out.}

\section{OpenMP Tasks} \label{Sec:OpenMP}

Initially, the cross-vendor OpenMP~\cite{spec} shared memory programming model 
focused on the execution of data parallelism by a cooperating team of threads, 
e.g., dividing the iterations of a loop among the threads. Version 3.0 of the 
OpenMP API specification introduced support for lightweight asynchronous tasks, 
designated by the application developer and scheduled onto the team of threads 
by the OpenMP run time implementation.  The \texttt{task} construct applied to 
a structured block of code creates an explicit task, and the \texttt{taskwait} 
construct waits for completion of all tasks generated by the current task.

Recursive task creations and synchronizations using the constructs result in 
an implicit directed acyclic graph (DAG) that allows both reasoning about and 
visualization of the program execution.  Figure~\ref{Fig:fib-graph-to-schedule}
shows some example code, a view of the task DAG, and a simplified execution 
schedule mapping the tasks to a team of two threads.  In the example, the Nth 
Fibonacci number is calculated by recursively generating tasks to calculate 
the (N-1)th and (N-2)th Fibonacci numbers.  The \texttt{taskwait} ensures that 
the child tasks have completed before their answers are combined to yield the 
final result.

\begin{figure*}
\begin{center}
\begin{tabular}{| c c c | c c c | c c c |}
\hline
 & 
\begin{lstlisting}
int fib(int n) 
{                    // A
  if (n < 2)
    return n;
  else
  {
    int x, y;
    #pragma omp task
      x = fib(n-1);  // B
    #pragma omp task
      y = fib(n-2);  // C
    #pragma omp taskwait
    return (x+y);    // D
  }
}
\end{lstlisting}
& & &
    \centering{
   \raisebox{-0.6in}{\includegraphics[scale=0.35]{fib-task-graph.pdf}}
    }
& & &
    \centering{
     \raisebox{-0.25in}{\includegraphics[scale=0.35]{fib-schedule.pdf}}
    }
 &  \\
\hline
\end{tabular}
\end{center}
\caption{Code, task graph, and schedule of a simple brute force recursive 
Fibonacci number calculation on two threads.}
\label{Fig:fib-graph-to-schedule}
\end{figure*}

The initial design of the OpenMP task model~\cite{ayguade09design} established 
a basic framework for asynchronous task parallel execution in OpenMP programs.
Subsequent versions of the OpenMP specification up to the current version 4.5 
have added additional new features to the tasking model.  The \texttt{depend} 
clause codifies data dependences among tasks, indicating that a data location 
is an input or output of a task.  The run time system ensures that a task is 
not scheduled until its input dependences are fulfilled.  The \texttt{taskloop} 
construct combines groups of independent loop iterations into explicit tasks, 
enabling composition of concurrent loop execution and independent explicit 
tasks within the same OpenMP parallel region. The \texttt{taskgroup} construct 
waits on not only all child tasks, but all descendent tasks, providing a deep 
synchronization. The \texttt{taskwait} construct allows the application to 
indicate a point at which the implementation may suspend the current task to 
work on other tasks, as may be desired for long-running tasks that generate 
many others.  The task concept was also leveraged to provide for asynchronous 
offload of data and computation to accelerators by applying the \texttt{nowait} 
clause to device constructs like the \texttt{target} construct to generate an 
asynchronous \textit{target task}.

Several clauses for the \texttt{task} construct aim to optimize execution of 
tasks but can be safely ignored by an implementation that chooses to do so. 
The \texttt{mergeable} clause allows the implementation to omit creation of a 
new data environment for a descendent of a task marked with the \texttt{final}
clause.  The \texttt{priority} clause assigns an integer priority to the task 
and recommends the prioritization of tasks with higher priority values.
The \texttt{untied} clause allows a task to be migrated between threads after 
suspension, which enables practical use of \textit{work-first scheduling} 
(suspending the parent task in favor of executing each child task immediately 
on the thread where it is generated).

\section{Implementation} \label{Sec:Implementation}

Tapir is implemented as an extension to the LLVM instruction set. Clang is a C
family compiler that has support for OpenMP extensions \cite{clang} and targets
LLVM. The existing Clang OpenMP implementation maps OpenMP constructs directly
to OpenMP runtime library calls, by wrapping a C statement in a
\texttt{CapturedStmt}. This replaces a C statement with a function call into
the OpenMP runtime library, along with a machine generated function who's body
contains that statement.   

For this work, we replaced a subset of the OpenMP implementation to generate
Tapir IR instead of vanilla LLVM IR with OpenMP runtime calls. Specifically, we
replace the two primary pragmas for task parallelism: \texttt{task\_spawn} and
\texttt{task\_wait}. By re-using code from Schardl et al. for code generation
of Cilk constructs, we were able to easily generate Tapir code for these OpenMP
pragmas. The ease with which this was completed is a testament to the quality of
the Tapir implementation. 

While we did implement the codegen for the \texttt{task\_spawn} and
\texttt{task\_wait} constructs, it's worth noting here that even for these
pragmas, the implementation is incomplete. Currently, any clauses modifying the
behaviour of the pragmas is ignored. Probably the most common semantics this
will change of variable semantics, e.g. \texttt{shared} vs. \texttt{private}.
Surprisingly, this had little effect on the correctness of the entire Barcelona
OpenMP task suite. Indeed, it is likely that fixing this issue would increase
the performance of this work marginally, due to reducing the number of memory
copies for variables declared \texttt{private} in OpenMP code. This property
that behaviour wasn't changed significantly is worth revisiting, and we do so
in Section~\ref{Sec:Discussion}. 

This work represented in this paper is only frontend implementation, which
means we use the only existing backend for Tapir. The existing backend
generates code that calls into the Cilk runtime, \texttt{libcilkrts}. This has
implications for adhering to the OpenMP specification. For example, environment
variables such as \texttt{OMP\_NUM\_THREADS} are ignored, replaced by
\texttt{CILK\_NWORKERS}.  We discuss some of these issues below, and return to
address others in the future work section. 

There are other issues that had to be overcome to run full OpenMP programs
using Tapir as described above. Generally, in addition the the \texttt{task} 
and \texttt{taskwait} pragmas, a program has to contain initialization OpenMP
pragmas in order to run parallel code. A standard pattern for building
task-parallel OpenMP programs is to insert a \texttt{parallel} pragma to start
the necessary hardware threads, followed immediately by a \texttt{single}
pragma to have only one thread continue on the specified statement. Then the
other threads will get work from spawned tasks, instead of implicitly executing
the same code in a data parallel style. For the purpose of this paper, we took
the shortcut of replacing these pragmas with no-ops. This worked well for all
examples except for one, in which after the \texttt{parallel} and
\texttt{single} pragmas the top level task was called with an unnecessary
\texttt{task} pragma.  Because the wait was implicit and unhandled by our
implementation, the simple fix was to remove the \texttt{task} pragma. This was
the only change required to the source code. Our temporary no-op shortcut, of
course, doesn't follow the OpenMP specification, and should be addressed by
future work. See Section~\ref{Sec:Future} for a further discussion how how the 
implementation can be improved.

All code used for the implementation is available at \\
\texttt{https://github.com/lanl/openmpir-clang}. The version used for this
paper is tagged as LLVM17.

\subsection{Example} \label{Sec:Example}

To better understand how the OpenMP to Tapir compiler works, we turn to the
\texttt{fib} example from Section~\ref{Sec:OpenMP}. In Figure~\ref{Fig:Example}
we show what Tapir code is generated.  

\begin{figure*}
\begin{tabular}{| c c c | c c c |}
\hline
 & 
\begin{lstlisting}
int fib(int n) 
{                    // A
  if (n < 2)
    return n;
  else
  {
    int x, y;
    #pragma omp task
      x = fib(n-1);  // B
    #pragma omp task
      y = fib(n-2);  // C
    #pragma omp taskwait
    return (x+y);    // D
  }
}
\end{lstlisting}
& & &
\begin{lstlisting}[language=llvm]
if.end:                                 
  detach within %syncreg, label %det.achd, label %det.cont

det.achd:                                 
  %2 = load i32, i32* %n.addr, align 4
  %sub = sub nsw i32 %2, 1
  %call = call i32 @fib(i32 %sub)
  store i32 %call, i32* %x, align 4
  reattach within %syncreg, label %det.cont

det.cont:                                   
  detach within %syncreg, label %det.achd1, label %det.cont4

det.achd1:                                
  %3 = load i32, i32* %n.addr, align 4
  %sub2 = sub nsw i32 %3, 2
  %call3 = call i32 @fib(i32 %sub2)
  store i32 %call3, i32* %y, align 4
  reattach within %syncreg, label %det.cont4

det.cont4:                                  
  sync within %syncreg, label %sync.continue
\end{lstlisting}
 &  \\
\hline
\end{tabular}

\caption{Compilation to Tapir}
\label{Fig:Example}
\end{figure*}

The listed Tapir LLVM IR code corresponds to lines 7 through 12 of the C source
code.  Upon entering the \texttt{else} branch, the code immediately detaches
the basic block corresponding to the call to \texttt{fib(n-1)}, labeled
\texttt{det.achd}. The continuation for the first call also immediately
detaches the basic block corresponding to the call of \texttt{fib(n-2)}.
Finally, the continuation for the second call immediately calls sync. This is
actually a nice example of code that will be optimized by Tapir. The LLVM code
shown in Figure~\ref{Fig:Example} is not optimized. With optimizataions turned
on, Tapir will replace a \texttt{detach} of a function call followed
immediately by \texttt{sync} with a simple function call, resulting in faster
code. 

\section{Evaluation} \label{Sec:Evaluation}

For evaluation we compared performance on the Barcelona OpenMP task suite
\cite{barcelona} to existing OpenMP implementations. The Barcelona OpenMP task
suite is a set of tests intended to test performance of OpenMP implementations
using both irregular and regular tasking. The benchmark suite has added an
unbalanced tree search benchmark since the paper was published. Unfortunately
we were unable to succesfully run it on any implementation, even for even
medium input sizes. We have therefore left it out of our evaluation. The
following descriptions are abridged versions taken verbatim from
\cite{barcelona}:

\begin{itemize}
\item \texttt{Alignment}: aligns all protein sequences from  an  input
file  against  every  other  sequence  using  the Myers and Miller 
algorithm. The alignments are scored and the best score for each pair is
provided as a result. The scoring method is  a  full  dynamic  programming
algorithm. It uses  a  weight matrix to score mismatches, and assigns
penalties for opening and extending gaps. The output is the best score for each
pair of them.
\item \texttt{FFT}: computes the one-dimensional Fast Fourier Transform
of a vector of n complex values using the Cooley-Tukey 
algorithm. This is a divide and conquer algorithm that  recursively  breaks
down a Discrete Fourier Transform (DFT) into many smaller DFT’s. In each of the
divisions multiple tasks are generated.
\item \texttt{Fibonacci}: computes the n'th fibonacci number using a  recursive
paralellization. While  not  representative  of  an efficient  fibonacci
computation  it  is  still  useful  because  it  is a simple test case of a
deep tree composed of very fine grain tasks.  
\item \texttt{Floorplan}: kernel computes the optimal floorplan distribution
of a number of cells. The algorithm gets an input file with  cell’s
description  and  it  returns  the  minimum  area  size which includes all
cells. This minimum area is found through a recursive branch and bound search.
We hierarchically generate tasks  for  each  branch  of  the  solution  space.
The  state  of  the algorithm needs to be copied into each newly created task
so they can proceed. This implies that additional synchronizations have been
introduced in the code to maintain the parent state alive.
\item \texttt{Health}: simulates the Columbian Health Care System. It
uses multilevel lists where each element in the structure  represents  a
village with  a  list  of  potential patients and one hospital. The hospital
has several double-linked lists representing the possible status of a patient
inside it (waiting, in assessment,   in   treatment   or   waiting   for
reallocation).  At  each time step  all  patients  are  simulated  according
with several probabilities (of getting sick, needing a convalescence treatment,
or  being  reallocated to  an  upper  level  hospital).  A  task  is  created
for  each  village being  simulated. Once the lower levels have been simulated
synchronization occurs. 
\item \texttt{NQueens}: computes  all  solutions  of  the n-queens
problem, whose objective is to find a placement for $n$ queens on an $n \;
\times \; n$ chessboard such that none of the queens attack any other. It uses
a backtracking search algorithm with pruning. A task is created for each step
of the solution.
\item \texttt{Sort}: sorts a random permutation of n 32-bit numbers with  a
fast  parallel  sorting  variation of  the  ordinary mergesort.  First, it
divides an array of elements in two halves, sorting  each half  recursively,
and  then  merging  the  sorted halves with a parallel divide-and-conquer
method rather than the  conventional  serial  merge.  Tasks are  used  for
each  split and merge. When the array is too small, a serial quicksort is used
so increase  the  task  granularity.  To  avoid  the overhead of  quicksort, an
insertion  sort  is  used  for  very  small  arrays (below a threshold of 20
elements).
\item \texttt{SparseLU}: computes an LU matrix factorization over
sparse matrices. A first level matrix is composed by pointers to  small
submatrices  that  may  not  be  allocated.  Due  to  the sparseness  of  the
matrix,  a  lot  of  imbalance  exists.  Matrix size and submatrix size can be
set at execution time. While a dynamic schedule can reduce the imbalance, a
solution with tasks parallelism seems to obtain better results. In each
of the sparseLU  phases,  a  task  is  created  for  each  block  of  the
matrix that is not empty.
\item \texttt{Strassen}: algorithm  uses  hierarchical  decomposition of a
matrix for multiplication of large dense matrices. Decomposition is done by
dividing each dimension of the matrix into  two  sections  of  equal size. For
each decomposition a task is created. 
\end{itemize}

There are two classes of input sizes for the benchmarks. Some, like Floorplan,
require input files. For each of these, we chose the largest provided. For the
benchmarks with a parameter to adjust the size of the input, we attempted to 
choose a parameter size so that the fastest implementation was on the order of
seconds. The one exception was for \texttt{Fibonacci}. Because, as discussed
further below, the Intel implementation required large stacksize, we stopped at
42. The exact inputs used are:

\begin{itemize}
\item \texttt{Alignment: -f prot.100.aa} 
\item \texttt{FFT: -n 335544320}
\item \texttt{Fibonacci: -n 42}
\item \texttt{Floorplan: -f input.20}
\item \texttt{Health: -f large.input}
\item \texttt{NQueens: -n 15}
\item \texttt{Sort -n 335544320}
\item \texttt{SparseLU: -n 100 -m 100}
\item \texttt{Strassen: -n 8192}
\end{itemize}

The Intel compiler required increasing the stacksize for \texttt{Fibonacci} and
\texttt{FFT}. The implementation seemed to use significantly more stack space
than the others, requiring setting \texttt{OMP\_STACKSIZE} to \texttt{256M} for
computing the 42nd Fibonacci number. \texttt{FFT} ran fine with an increase to
\texttt{16M}.  

For other implementations, we compare to GCC 7.1, Clang 4.0.1, and Intel
17.0.0. The machine used is a two socket Intel Xeon E5-2683 v3 machine with
132GB of memory, running Linux 4.11.4. Each Xeon has 16 cores running at
2.1GHz, with 20M of L3 cache. All benchmarks were run using all 64
hyper-threads. 32 thread tests were run as a sanity check and showed
qualitatively similar results. It's worth noting that each of the other
implementations comes with it's own runtime. In this sense, performance is a
function of both the compiler and any optimizations it's able to perform, and
the runtime.  Understanding the interaction of these two parts is non-trivial,
as we will see when discussing results. Each compiler was run with the flags
\texttt{-O3 -fopenmp}. Our compiler was also run with \texttt{-ftapir} to
enable Tapir instructions.

As mentioned in the Section~\ref{Sec:Implementation}, the gaps in the implementation
changed program behaviour in a couple cases. In the case of \texttt{FFT}, we were
forced to remove an unecessary \texttt{task} pragma, as the current implementation
doesn't insert the implicit barrier at the end of a \texttt{parallel} region. This 
was a one line fix in the benchmark, and would be fixed properly by handling
the \texttt{parallel} and \texttt{single} pragmas correctly in our implementation. 

The second change in program behaviour due to our incomplete implementation was
caused by the lack \texttt{critical} pragma and \texttt{atomic} pragmas. This
issue showed up in the \texttt{Floorplan} benchmark. The \texttt{critical} pragma
ensures that only one thread can executed the referred statement at a time. 
This should be fixable in the implementation by adding a simple code-localized
synchronization generation in the IR. Similarly, the \texttt{atomic} pragma 
can be addressed by retaining a pointer to the shared stack variable, much like
existing OpenMP implementations. It is worth noting that this behaviour was
non-deterministic, and that roughly half of the time the \texttt{Floorplan}
still returned correct results. As we will see, this makes for an interesting
trade-off given the performance increase witnessed.

We set a timeout of 10 minutes, as at least one implementation was always
finishing within 10 seconds, anything running more than 60 times slower becomes
irrelevant. Due to time constraints, correctness checks were not performed on
every run, so it is possible some non-determinism was missed. 

Each variant was run 10 times, with the height of the bars representing the
mean and the error bars representing standard deviation. In cases where there
was timeout or segmentation fault, no time is reported, and the run is marked
as faulty. 

\section{Results} \label{Sec:Results}

In this section we discuss the results of running our evaluation on our
implementation, and how its performance compared to the existing
implementations listed in Section~\ref{Sec:Evaluation}. See
Figures~\ref{Fig:results} and \ref{Fig:results2} for a simple visualization of
the performance. Each graph represents a single benchmark from the previously
enumerated Barcelona benchmarks, and each bar represents one of the
implementations. 

\begin{figure*}
\begin{multicols}{2}
  \includegraphics[width=\linewidth]{alignment.pdf} \par
  \includegraphics[width=\linewidth]{fib.pdf} 
  \includegraphics[width=\linewidth]{fft.pdf} \par
  \includegraphics[width=\linewidth]{floorplan.pdf} 
  \includegraphics[width=\linewidth]{sort.pdf} \par
  \includegraphics[width=\linewidth]{nqueens.pdf}
\end{multicols}
\caption{Barcelona OpenMP Task Suite Results}
\label{Fig:results}
\end{figure*}

\begin{figure*}
\begin{multicols}{2}
  \includegraphics[width=\linewidth]{health.pdf} \par
  \includegraphics[width=\linewidth]{sparselu.pdf} 
\end{multicols}
\centering
\includegraphics[width=0.45\linewidth]{strassen.pdf}
\caption{Barcelona OpenMP Task Suite Results (continued)}
\label{Fig:results2}
\end{figure*}

As mentioned earlier on \texttt{Floorplan} our implementation was sometimes
giving incorrect results. It is difficult to tell, but the time to completion
is non-zero. In fact, while ICC and Clang finished in roughly 1-2 seconds, the
our Tapir implementation was finishing in 0.02 seconds, and computing the
correct result roughly half the time. This raises interesting questions on the
cost/benefit relation of the OpenMP behaviour that our implementation is
missing. We'll discuss this further in Section~\ref{Sec:Discussion}.

GCC was the only culprit for timeouts. Recall that the timeout was set at 10
minutes, so any timeout means GCC was running at least approximately 60 times
slower than the fastest implementation. For example, on the \texttt{FFT}
benchmark, while our Tapir implementation was running in under 5 seconds, the
GCC implementation was timing out at 600 seconds, so was at least 100 times
slower. We've left GCC results off of the graphs for the three failure cases, 
\texttt{FFT}, \texttt{Fibonacci}, and \texttt{NQueens}.

Putting failures aside, performance for our implementation is quite strong,
generally outperforming or matching the best of existing implementations. For
benchmarks where the overhead to work ratio is high is where the Tapir
implementation really shines. For example, for the \texttt{Fibonacci}
benchmark, our implementation finishes significantly faster than any of the
others. While ICC failed for that sized input, we did test with smaller inputs
and found its similarly outpaced by the Tapir implementation.  \texttt{NQueens}
and \texttt{FFT} show similar behaviour, to lesser degrees. In contrast,
benchmarks that have a lower overhead to work ratio unsurprisingly differ less
in performance. For example,  

\subsection{Profiling}

In this section we attempt to understand \emph{why} the performance varies in
the ways it does. While we can't hope to figure out every discrepancy in
performance, we can hope to get some ideas for why the performance of our
implementation is generally better, and where each of the implementations is
spending its time.

Our primary tool for this task will be performance counters. It's worth noting
that some information is difficult to infer from performance counters, such as
sources of contention in runtimes, reasons for memory locality issues, etc.
Still, it will give us some insight into performance bottlenecks.

Before we begin understanding why, it's worth noting again that the scope of the 
work that this paper is responsible for is only the frontend. All performance
gains due to the existing Tapir backend and Cilk runtime we sadly cannot claim
credit for. 

We can break reasons for performance discrepancies into a few categories: 

\begin{itemize}
\item Front-end code generation
\item IR Optimizations
\item Runtime efficiency
\end{itemize}

While we would like to distinguish between these, it is difficult to do so with
only performance counter information. A more thorough investigation would
require careful knowledge of each of the frontends and runtimes. One thing we
can guess at is that despite the focus of the paper, \emph{IR optimisations are
unlikely to be the cause of performance improvements}. Our evidence for this
comes from Shardl et al. \cite{tapir}. While some IR optimisations have been
implemented, they seem to have significantly more effect on parallel loop code,
while the tasking code remains relatively unaffected by IR optimizations
\cite{tapir}. 

There are two cases we'd like to look at using the profiler. The first is a 
case when there is a large difference in runtime between implementations. For 
this case, our hypothesis is that in these cases the slower implementations
are, for some reason, spending more time in the runtime. As mentioned above, 
it is difficult to understand exactly why an implementation is spending more 
time in the runtime without further investigation, but it is a useful 
sanity check nonetheless. For this case, we turn to the \texttt{Fib} benchmark.
Indeed, we get roughly the following breakdowns of work to overhead ratio according
to the Linux \texttt{perf} tool:

\begin{itemize}
\item \texttt{tapir: ~30\% runtime overhead, ~50\% work, ~20\% other}
\item \texttt{clang: ~85\% runtime overhead, ~5\% work, ~10\% other}
\item \texttt{icc: ~85\% runtime overhead, ~5\% work, ~10\% other}
\item \texttt{gcc: ~100\% runtime overhead, ~0\% work, ~0\% other}
\end{itemize}

This confirms our suspicions that most of the slowdown incurred is overhead in
the runtime. For \texttt{tapir}, the overhead is relatively small given the
size of the work chunks, for \texttt{clang} and \texttt{icc} it is
significantly larger, while for \texttt{gcc} it is completely overwhelming.
This does let us rule out the possibility that somehow the only the codegen for
other implementations is the culprit: it is almost certainly at least partially
the runtime implementation.  

For the cases where performance is close between implementations, like
\texttt{sparselu}, we expect runtimes to be dominated by actual task work. This 
expectation is confirmed by investigating performance counters for \texttt{sparselu}:
every implementation spends roughly 90\% of it's time doing work in the task
bodies. 

\section{Discussion} \label{Sec:Discussion}

In this section we discuss the relevance of the results and this work in general
in the context of the literature. As discussed in Section~\ref{Sec:Introduction}, 
there have been many programming models for implementing parallel programs. While
this paper does not take a stance on which one \emph{should} be used, it does
accept that OpenMP is currently a widely used model, potentially the most
widely used model that has a notion of tasks. 

The goal of this work is to show that a single intermediate representation can be
used to implement multiple programming models, with good performance. Schardl et 
al. showed that Tapir works well as an IR for Cilk, and surmised that it could
be an effective IR for a large subset of OpenMP. This work has taken the first
step in that direction, showing that it can work well as an IR for OpenMP tasks. 

One interesting aspect of this work is that it compiles a language extension intended
for one runtime (OpenMP runtimes) to use another (the Cilk runtime). While in many
ways this is a weakness of this work, it does open the possibility of doing this 
in a general way. Having a real IR to work with enables, in the same way that LLVM
does for serial code, the possibility of multiple input parallel languages and
multiple output parallel runtimes. While the OpenMP spec's requirement to
have access to runtime routines complicates things, for many programming models
it seems at least conceivable to map them onto multiple different runtimes. This
opens the possibility of mixing programming models at compile time by targeting
the same runtime. For example, if both OpenMP and Cilk targeted Tapir
instructions, one could have a large application where some parts were written
using Cilk, while other used OpenMP, and the result would not suffer the
incompatibility problems often seen today. 

The surprising fact that program behaviour wasn't changed significantly is
worth discussing here. We surmise that it is likely due to the variable
semantics borrowed from the Cilk codegen and their relation to the OpenMP
specification of semantics. In particular, \emph{OpenMP doesn't specify any
memory model for shared variables}. This results in the Cilk style of only
writing to a shared variable at the end of a parallel section as a perfectly
valid implementation of OpenMP's specification. In contrast, existing OpenMP
implementations often write to shared variables on every write. This is a
potential for performance discrepancy due to differing implementation-defined
behaviour, and we will return to it when discussing performance on the
Barcelona OpenMP task suite. There is of course the fact that technically, by
copying the value of variables back to the surrounding context even for
\texttt{private} variables, one isn't following the specification. We surmise
that in practice this hasn't had an effect due to the fact that
\texttt{private} variables are generally used for performance, rather than
their semantic properties. 

\subsection{Future Work} \label{Sec:Future}

One of the important questions for this work is whether or not the Tapir
instructions can be extended to cover the full OpenMP semantics. As shown
in this paper, even the full semantics of OpenMP tasks weren't covered. Runtime
calls aside, implementing the full semantics of OpenMP pragmas would be a 
significant undertaking.

As two examples, let us consider the problematic \texttt{atomic} and
\texttt{critical} pragmas from the \texttt{Floorplan} benchmark. The 
\texttt{atomic} pragma would likely not be too difficult to translate
from existing OpenMP codegen. One would have to simply keep around and then
modify the contents of the original stack pointer to the shared variable. The
\texttt{critical} section would likely be a little more difficult. Still, it
should be possible to modify existing OpenMP code to block on that section. 

Still, whether or not it's \emph{possible} to implement these extra features
is a separate question from whether or not it's \emph{useful}. If implementing
extra features require using special synchronization primitives, or calls into
runtimes, or any other tool other than the Tapir instructions, then the
compiler cannot reason about code using those features, and many of the
advantages we've discussed become moot, e.g. backend compatibility, generic 
optimization, etc. Features like the OpenMP \texttt{atomic} pragma don't 
require any changes to control flow, and therefore are likely to work well 
with Tapir analyses and optimizations. On the other hand features like OpenMP's
\texttt{critical} section \emph{do} require changes to control flow that seem
challenging at best to map onto Tapir's instructions. Furthermore, even if it
is possible to implement features using the Tapir instructions, it may require 
such radical entangling of the code that even if analysis is technically
possible, it becomes infeasible. 

While OpenMP and Cilk have similar fork-join style semantics that Tapir
implements naturally, many allow a more flexible mechanism for synchronization:
futures \cite{qthreads, chapel, hpx}. The primary difference between the
fork-join parallelism represented by Tapir and these approaches is that
child tasks can outlive their parents. Figuring out how to represent this class
of parallelism in Tapir represents a significant challenge for future work.

\subsection{Related Work} \label{Sec:Related}

There has been significant work on internal representations for parallel
programs. These can be roughly broken into four categories. 

First, compilers can attach metadata to an existing IR. For example, LLVM has a
parallel loop metadata construct \cite{}. This has the benefit of being
flexible and requiring minimal work, but historically can be lost during
optimizations, and any code movement can break the semantics. The flexibility
of this approach can also be viewed as a negative, as it can compromise
what can otherwise be a simple, well defined semantics for the IR. 

A second approach is to use intrinsic functions to define parallel tasks
\cite{ares}. This, like the metadata approach, has the advantage of being
flexible and relatively easy to implement, but at the cost of being less well
defined. There has recently been proposals to standardize intrinsics extensions
for LLVM to represent parallelism, but we would caution that a large, flexible
set of intrinsics being added to an already only-partially defined
\cite{verillvm} language could make reasoning about parallel programs, even
in an IR settings, infeasible. 

A third approach is to have a separate instruction set for parallelism. This is
the approach taken by projects like HPIR \cite{zhao2011intermediate}, SPIRE
\cite{khaldi2012spire}, and INSPIRE \cite{jordan2013inspire}. This approach has
the downside of being unable to re-use existing optimization and analysis
infrastructure of an existing IR. 

Finally, the last approach, and that taken by Tapir, is to implement parallel 
instructions as an extension of an existing IR. This allows for integration
into existing analyses and optimizations. In the case of Tapir, this allows one
to leverage years of development into program analysis and optimization,
extending only where necessary. 

While there is general agreement that there needs to be IR support for
parallelism in parallel programs, there isn't consensus on which of these
approaches is best. Approaches that are easier at first, such as metadata
or intrinsic approaches, are similarly easy to extend to be unwieldy. There is
a reason that it's difficult to add instructions to LLVM: any added IR
construct needs to be considered in many locations. From debugging by printing
out IR, to case analysis for different control flow constructs, having 
a proper set of instructions to denote parallelism has a lot of advantages. 
Additionally, any hope of having a formal semantics for an IR depends on a 
simple, concise set of parallelism constructs, something that metadata and
intrinsics approaches can easily avoid. 

\section{Conclusion} \label{Sec:Conclusion}
We have shown that Tapir is an excellent IR target for OpenMP tasks. While not
all of the semantics are covered, we have a path forward for many of them. We have
shown that compiling to Tapir instructions allows for a straightforward compilation of 
OpenMP tasking programs to use the Cilk runtime system. We've also shown that this
combination leads to better performance than existing OpenMP tasking implementations
on the Barcelona OpenMP Tasking benchmark suite. Moving forward, we hope efforts like
this one help to reduce the fragmentation of parallel runtimes, as well as make
it easier to write optimization for parallel programs. 

\section*{Acknowledgments}
Sandia National Laboratories is a multimission laboratory managed and operated 
by National Technology and Engineering Solutions of Sandia, LLC., a wholly 
owned subsidiary of Honeywell International, Inc., for the U.S. Department of 
Energy’s National Nuclear Security Administration under contract DE-NA-0003525.


This work was supported by the Director, Office of Advanced Scientific
Computing Research, Office of Science, of the United States Department
of Energy, under the guidance of Dr. Sonia Sachs. Los Alamos National
Laboratory is operated by Los Alamos National Security LLC for the U.S.
Department of Energy under contract DE-AC52-06NA25396.

This research was supported by US National Science Foundation under CRII
ACI 1565338, CNS 1527076, and CNS 1217948. Los Alamos National
Laboratory is operated by Los Alamos National Security LLC for the US
Department of Energy under contract DE-AC52-06NA25396.

Work supported by the Advanced Simulation and Computing program of the
U.S. Department of Energy's NNSA\@.  Los Alamos National Laboratory is
managed and operated by Los Alamos National Security, LLC (LANS), under
contract number DE-AC52-06NA25396 for the Department of Energy’s
National Nuclear Security Administration (NNSA).

\bibliographystyle{ACM-Reference-Format}
\bibliography{annotated}

\end{document}
